# SST-2 Sentiment Analysis: BERT vs fastText

SST-2 (Stanford Sentiment Treebank)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸæ„Ÿæƒ…åˆ†æã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€**BERT**ã¨**fastText**ã®2ã¤ã®ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¯”è¼ƒã™ã‚‹è‡ªç„¶è¨€èªå‡¦ç†ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚

## ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€ä»¥ä¸‹ã‚’å®Ÿæ–½ã—ã¾ã™ï¼š

- **2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡**
  - **BERT** (bert-base-uncased): äº‹å‰å­¦ç¿’æ¸ˆã¿Transformerãƒ¢ãƒ‡ãƒ«
  - **fastText**: é«˜é€Ÿãªå˜èªåŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡å™¨

- **è©³ç´°ãªæ€§èƒ½æ¯”è¼ƒåˆ†æ**
  - Accuracyã€Precisionã€Recallã€F1ã‚¹ã‚³ã‚¢
  - æ··åŒè¡Œåˆ—
  - å­¦ç¿’æ›²ç·š

- **ã‚¨ãƒ©ãƒ¼åˆ†æ**
  - æ–‡ç« é•·ã«ã‚ˆã‚‹æ€§èƒ½å·®
  - å¦å®šè¡¨ç¾ã¸ã®å¯¾å¿œåŠ›
  - ãƒ¢ãƒ‡ãƒ«é–“ã®åˆ¤æ–­ã®ç›¸é•

## ğŸ—‚ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
nlp_final_project/
â”œâ”€â”€ main.py                    # ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ data_loader.py             # SST-2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†
â”œâ”€â”€ bert_classifier.py         # BERTåˆ†é¡å™¨ã®å®Ÿè£…
â”œâ”€â”€ fasttext_classifier.py     # fastTextåˆ†é¡å™¨ã®å®Ÿè£…
â”œâ”€â”€ evaluate.py                # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æ©Ÿèƒ½
â”œâ”€â”€ analysis.py                # è©³ç´°åˆ†æã¨å¯è¦–åŒ–
â”œâ”€â”€ error_analysis.py          # ã‚¨ãƒ©ãƒ¼åˆ†æ
â”œâ”€â”€ requirements.txt           # ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
â””â”€â”€ outputs/                   # å®Ÿè¡Œçµæœã®å‡ºåŠ›å…ˆ
    â”œâ”€â”€ models/                # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    â”œâ”€â”€ figures/               # å¯è¦–åŒ–ã‚°ãƒ©ãƒ•
    â””â”€â”€ results/               # è©•ä¾¡çµæœCSV
```

## ğŸš€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ç’°å¢ƒæ§‹ç¯‰

**Python 3.8ä»¥ä¸Š**ãŒå¿…è¦ã§ã™ã€‚ä»®æƒ³ç’°å¢ƒã®ä½œæˆã‚’æ¨å¥¨ã—ã¾ã™ï¼š

```bash
# ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
python3 -m venv venv

# ä»®æƒ³ç’°å¢ƒã®æœ‰åŠ¹åŒ–
source venv/bin/activate  # Linux/Mac
# ã¾ãŸã¯
venv\Scripts\activate     # Windows
```

### 2. ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install -r requirements.txt
```

**ä¸»ãªä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:**
- `torch>=2.0.0` - PyTorchæ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- `transformers>=4.30.0` - Hugging Face Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- `datasets>=2.14.0` - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- `fasttext>=0.9.2` - fastTextãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- `scikit-learn>=1.3.0` - æ©Ÿæ¢°å­¦ç¿’ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
- `matplotlib`, `seaborn` - ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–

### 3. GPUç’°å¢ƒï¼ˆæ¨å¥¨ï¼‰

BERTã®å­¦ç¿’ã«ã¯GPUã®ä½¿ç”¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚GPUç’°å¢ƒã®ç¢ºèªï¼š

```bash
python -c "import torch; print(torch.cuda.is_available())"
```

## ğŸ“– ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªå®Ÿè¡Œ

```bash
python main.py
```

### ã‚ˆãä½¿ã†ã‚ªãƒ—ã‚·ãƒ§ãƒ³

#### ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§é«˜é€Ÿå®Ÿè¡Œï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰

```bash
python main.py --use_subset --subset_ratio 0.1
```

#### ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’å®Ÿè¡Œ

```bash
# BERTã®ã¿
python main.py --skip_fasttext

# fastTextã®ã¿
python main.py --skip_bert
```

#### å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿

```bash
python main.py --load_bert --load_fasttext
```

#### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´

```bash
python main.py \
  --bert_epochs 5 \
  --bert_batch_size 32 \
  --bert_learning_rate 2e-5 \
  --fasttext_epochs 30
```

#### ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹æ€§èƒ½å¤‰åŒ–ã®åˆ†æ

```bash
python main.py --analyze_data_size
```

### å…¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¸€è¦§

```
ä½¿ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³:
  --output_dir DIR          å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./outputs)
  --seed SEED               ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 42)
  --use_subset              ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
  --subset_ratio RATIO      ã‚µãƒ–ã‚»ãƒƒãƒˆã®å‰²åˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1)
  
  BERTè¨­å®š:
  --skip_bert               BERTã®å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—
  --load_bert               ä¿å­˜æ¸ˆã¿BERTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
  --bert_epochs N           ã‚¨ãƒãƒƒã‚¯æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3)
  --bert_batch_size N       ãƒãƒƒãƒã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 16)
  --bert_learning_rate LR   å­¦ç¿’ç‡ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2e-5)
  --bert_max_length N       æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•· (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128)
  
  fastTextè¨­å®š:
  --skip_fasttext           fastTextã®å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—
  --load_fasttext           ä¿å­˜æ¸ˆã¿fastTextãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
  --fasttext_epochs N       ã‚¨ãƒãƒƒã‚¯æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 25)
  --fasttext_lr LR          å­¦ç¿’ç‡ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1)
  --fasttext_dim N          åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100)
  
  åˆ†æ:
  --analyze_data_size       ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹æ€§èƒ½å¤‰åŒ–ã‚’åˆ†æ
```

## ğŸ“ˆ å‡ºåŠ›çµæœ

å®Ÿè¡Œå¾Œã€`outputs/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä»¥ä¸‹ãŒä¿å­˜ã•ã‚Œã¾ã™ï¼š

### 1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (`outputs/models/`)
- `bert_model/` - å­¦ç¿’æ¸ˆã¿BERTãƒ¢ãƒ‡ãƒ«
- `fasttext_model.bin` - å­¦ç¿’æ¸ˆã¿fastTextãƒ¢ãƒ‡ãƒ«

### 2. å¯è¦–åŒ–ã‚°ãƒ©ãƒ• (`outputs/figures/`)
- `model_comparison.png` - ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼ˆAccuracy, F1ãªã©ï¼‰
- `bert_confusion_matrix.png` - BERTæ··åŒè¡Œåˆ—
- `fasttext_confusion_matrix.png` - fastTextæ··åŒè¡Œåˆ—
- `length_comparison.png` - æ–‡ç« é•·åˆ¥ã®æ€§èƒ½æ¯”è¼ƒ
- `negation_comparison.png` - å¦å®šè¡¨ç¾ã¸ã®å¯¾å¿œåŠ›æ¯”è¼ƒ

### 3. è©•ä¾¡çµæœ (`outputs/results/`)
- `model_comparison.csv` - ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒè¡¨
- `dataset_stats.csv` - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ
- `error_summary.txt` - ã‚¨ãƒ©ãƒ¼åˆ†æã‚µãƒãƒªãƒ¼
- `*_high_conf_errors.csv` - é«˜ä¿¡é ¼åº¦ã‚¨ãƒ©ãƒ¼äº‹ä¾‹
- `*_length_analysis.csv` - æ–‡ç« é•·åˆ¥åˆ†æçµæœ
- `*_negation_examples.csv` - å¦å®šè¡¨ç¾ã®åˆ†æçµæœ
- `disagreement_*.csv` - ãƒ¢ãƒ‡ãƒ«é–“ã®åˆ¤æ–­ã®ç›¸é•äº‹ä¾‹

## ğŸ“Š å®Ÿé¨“çµæœã®ä¾‹

å®Ÿè¡Œä¾‹ï¼ˆã‚µãƒ–ã‚»ãƒƒãƒˆ10%ä½¿ç”¨ï¼‰ï¼š

| Model    | Accuracy | Precision | Recall | F1 Score |
|----------|----------|-----------|--------|----------|
| BERT     | 0.89     | 0.90      | 0.88   | 0.89     |
| fastText | 0.82     | 0.81      | 0.83   | 0.82     |

**ä¸»ãªç™ºè¦‹:**
- BERTã¯æ–‡è„ˆç†è§£ã«å„ªã‚Œã€è¤‡é›‘ãªè¡¨ç¾ã‚„å¦å®šæ–‡ã«å¼·ã„
- fastTextã¯å­¦ç¿’ãƒ»æ¨è«–ãŒé«˜é€Ÿã§ã€å˜ç´”ãªæ–‡ã«å¯¾ã—ã¦ã¯ååˆ†ãªæ€§èƒ½
- é•·æ–‡ã«ãªã‚‹ã»ã©BERTã®å„ªä½æ€§ãŒé¡•è‘—

## ğŸ” æŠ€è¡“è©³ç´°

### BERTã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- **ãƒ¢ãƒ‡ãƒ«**: `bert-base-uncased`ï¼ˆHugging Faceï¼‰
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: 12å±¤Transformerã€768æ¬¡å…ƒ
- **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼**: WordPieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
- **æœ€é©åŒ–**: AdamWã€å­¦ç¿’ç‡2e-5
- **Fine-tuning**: åˆ†é¡å±¤ã®ã¿è¿½åŠ ã—ã¦å…¨ä½“ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### fastTextã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: å˜èªåŸ‹ã‚è¾¼ã¿ã®å¹³å‡ + ç·šå½¢åˆ†é¡å™¨
- **ç‰¹å¾´**: ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã®æ´»ç”¨ï¼ˆcharacter n-gramsï¼‰
- **æœ€é©åŒ–**: SGDã€å­¦ç¿’ç‡0.1
- **åˆ©ç‚¹**: é«˜é€Ÿãªå­¦ç¿’ãƒ»æ¨è«–ã€OOVï¼ˆæœªçŸ¥èªï¼‰ã¸ã®å¯¾å¿œåŠ›

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- **SST-2**: Stanford Sentiment Treebank v2
- **ã‚¿ã‚¹ã‚¯**: äºŒå€¤æ„Ÿæƒ…åˆ†é¡ï¼ˆPositive/Negativeï¼‰
- **ãƒ‡ãƒ¼ã‚¿é‡**: 
  - Training: 67,349ã‚µãƒ³ãƒ—ãƒ«
  - Validation: 872ã‚µãƒ³ãƒ—ãƒ«
  - Test: 1,821ã‚µãƒ³ãƒ—ãƒ«ï¼ˆãƒ©ãƒ™ãƒ«ãªã—ï¼‰

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Python 2.7ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚¨ãƒ©ãƒ¼

```bash
# Python 3ã‚’æ˜ç¤ºçš„ã«ä½¿ç”¨
python3 -m pip install -r requirements.txt
python3 main.py
```

### importlib_metadataã®ã‚¨ãƒ©ãƒ¼ï¼ˆPython 3.8ï¼‰

```bash
pip install --upgrade 'importlib-metadata>=6.0.0'
```

### GPU/CUDAã®ã‚¨ãƒ©ãƒ¼

CPUã®ã¿ã§å®Ÿè¡Œã™ã‚‹å ´åˆã€BERTã®å­¦ç¿’ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼š

```bash
# CPUã§å®Ÿè¡Œï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰
CUDA_VISIBLE_DEVICES="" python main.py --use_subset --subset_ratio 0.1
```

### ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦ãã ã•ã„ï¼š

```bash
python main.py --bert_batch_size 8
```

## ğŸ“ å¼•ç”¨

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ¢ãƒ‡ãƒ«ï¼š

```bibtex
@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
  booktitle={Proceedings of EMNLP},
  year={2013}
}

@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{joulin2016fasttext,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.01759},
  year={2016}
}
```

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å­¦è¡“ãƒ»æ•™è‚²ç›®çš„ã§ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ‘¤ ä½œæˆè€…

- GitHub: [@KeijiroOnodera](https://github.com/KeijiroOnodera)
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: [nlp_final_project](https://github.com/KeijiroOnodera/nlp_final_project)

## ğŸ™ è¬è¾

- Hugging Face Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- Facebook Research fastTextãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- Stanford NLP Groupï¼ˆSST-2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
